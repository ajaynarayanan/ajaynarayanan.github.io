<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ajaynarayanan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ajaynarayanan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-24T02:43:38+00:00</updated><id>https://ajaynarayanan.github.io/feed.xml</id><title type="html">Ajay Narayanan Sridhar</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">InsectAgent: mobile insect recognition with VLM and on-device vision</title><link href="https://ajaynarayanan.github.io/blog/2025/insect-agent/" rel="alternate" type="text/html" title="InsectAgent: mobile insect recognition with VLM and on-device vision"/><published>2025-10-23T00:00:00+00:00</published><updated>2025-10-23T00:00:00+00:00</updated><id>https://ajaynarayanan.github.io/blog/2025/insect-agent</id><content type="html" xml:base="https://ajaynarayanan.github.io/blog/2025/insect-agent/"><![CDATA[<p>InsectAgent was presented at ISVLSI 2025 <d-cite key="zhao2025insectagent"></d-cite>. This post gives a brief overview of <strong>InsectAgent</strong>, a small, on-device iOS app that recognizes insects and only “calls in” extra reasoning using a <strong>Multimodal Large Language Model (MLLM/VLM)</strong> when the photo is tricky.</p> <p>This post is designed for readers with little to no coding experience. We’ll keep things visual and plain-language, and focus on the <em>why</em> and <em>how</em> of the app’s hybrid pipeline on iPhone.</p> <p>I hope you find this exploration both accessible and useful!</p> <h2 id="introduction">Introduction</h2> <blockquote> <p>Field reality: cell coverage can be patchy (or nonexistent) around traps and plots. When your model depends on the network, everything from retries to latency becomes unpredictable. That’s why <strong>running locally</strong> matters here, beyond privacy alone.</p> </blockquote> <p>Spotting and identifying insects early can protect crops, reduce chemical sprays, and improve our understanding of seasonal dynamics. The challenge on mobile is achieving <strong>good accuracy with reliable, offline behavior</strong>.</p> <p><strong>InsectAgent</strong> takes a pragmatic approach:</p> <ol> <li><strong>Fast local guess</strong>: a compact image classifier runs on-device and proposes likely species.</li> <li><strong>Think harder only if needed</strong>: a lightweight multimodal model (FastVLM <d-cite key="vasu2025fastvlm"></d-cite>) compares the photo against short species cues to resolve tough look-alikes.</li> </ol> <p>The result feels snappy on easy photos and smarter on ambiguous ones—<strong>no cloud round-trips</strong> and no dependency on spotty field networks.</p> <hr/> <h2 id="what-is-insectagent">What is InsectAgent?</h2> <p><strong>InsectAgent</strong> is an iOS app (Swift/Xcode) (<a href="https://github.com/ajaynarayanan/InsectAgent">code</a>) that demonstrates <strong>hybrid insect recognition</strong> on modern iPhones:</p> <ul> <li>A <strong>ResNet-18</strong> classifier <d-cite key="he2016deep"></d-cite> provides <strong>top-k</strong> candidates quickly.</li> <li><strong>FastVLM</strong> <d-cite key="vasu2025fastvlm"></d-cite>, a small multimodal LLM, is engaged <strong>only when the classifier isn’t confident</strong>, using brief, visual-first species descriptions (color patterns, wing venation, antenna shape, etc.) to reason about which candidate fits the image best.</li> </ul> <p><strong>Why this design?</strong></p> <ul> <li>Many photos are “easy wins”, the correct class is confidently top-1.</li> <li>For borderline cases, the right class often sits <em>in</em> the top-k; a little <strong>text-guided reasoning</strong> helps pick the winner.</li> <li>Doing this <strong>conditionally</strong> keeps the common case fast while improving the hard cases.</li> </ul> <hr/> <h2 id="insectagent-pipeline">InsectAgent Pipeline</h2> <div class="col-sm mt-3 mt-md-0 text-center"> <img src="/assets/blogs/insect_agent/insect_agent_pipeline.png" class="img-fluid rounded z-depth-1" alt="InsectAgent pipeline"/> <div class="caption mt-2 text-center"> Fig. 1: InsectAgent pipeline. A fast vision model proposes candidates; a VLM is used, only when needed, to compare the image against compact species cues. </div> </div> <h3 id="pipeline-components-at-a-glance">Pipeline components (at a glance)</h3> <p><strong>1) Vision model → top-k logits</strong> A compact CNN (say, ResNet-18) produces logits and a ranked list of species candidates. If the <strong>top-1 confidence ≥ τ</strong> (a configurable threshold), we return that label immediately.</p> <p><strong>2) Dynamic information augmentation (conditional)</strong> If confidence is <strong>low</strong>, the system fetches <strong>short, visual-first cue cards</strong> for the top-k species (e.g., “yellow-black banding,” “hind-wing ocelli,” “clubbed antennae”) from a knowledge base.</p> <p><strong>3) Multimodal reasoning</strong> A MLLM compares the input image with those cue cards, weighs evidence, and selects the best match—often correcting near-misses among visually similar species.</p> <hr/> <h2 id="results">Results</h2> <h3 id="on-device-variants-fastvlm-on-iphone-16-pro-8-gb-ram">On-device variants (FastVLM<d-cite key="vasu2025fastvlm"></d-cite>) on iPhone 16 Pro (8 GB RAM)</h3> <p><em>100 random insect images from IP102<d-cite key="wu2019ip102"></d-cite>; average end-to-end latency includes conditional runs when triggered. “Out of Memory” means the variant exceeded practical memory limits for our setup.</em></p> <table> <thead> <tr> <th style="text-align:left;">Method</th> <th style="text-align:right;">Latency (s)</th> <th style="text-align:right;">Accuracy (%)</th> </tr> </thead> <tbody> <tr> <td>ResNet-18<d-cite key="he2016deep"></d-cite> only</td> <td style="text-align:right;">0.150</td> <td style="text-align:right;">46.47</td> </tr> <tr> <td>ResNet-18 + FastVLM 0.5B</td> <td style="text-align:right;">3.016</td> <td style="text-align:right;">51.18</td> </tr> <tr> <td>ResNet-18 + FastVLM 1.5B</td> <td style="text-align:right;">7.205</td> <td style="text-align:right;">57.06</td> </tr> <tr> <td>ResNet-18 + FastVLM 7B</td> <td style="text-align:right;">Out of Memory</td> <td style="text-align:right;">Out of Memory</td> </tr> </tbody> </table> <p><strong>Notes.</strong></p> <ul> <li>The threshold <strong>τ</strong> governs how often the VLM is invoked: higher τ → more VLM calls (better accuracy, higher latency); lower τ → fewer calls (faster, slightly lower accuracy).</li> <li>The <strong>0.5B</strong> and <strong>1.5B</strong> VLMs provide a meaningful accuracy bump over vision-only while remaining feasible on device.</li> <li>For broader benchmarks and ablations (e.g., different top-k sizes, cue length), see the paper <d-cite key="zhao2025insectagent"></d-cite>.</li> </ul> <hr/> <h2 id="video-demo">Video demo</h2> <div class="col-sm mt-3 mt-md-0 text-center"> <img src="/assets/blogs/insect_agent/insect_agent_demo.gif" class="img-fluid rounded z-depth-1" alt="InsectAgent demo"/> <div class="caption mt-2 text-center"> Fig. 2: Demo of the conditional pipeline in action on an iPhone 16 Pro. </div> </div> <hr/> <h2 id="conclusion--next-steps">Conclusion &amp; next steps</h2> <p>Pairing a <strong>fast vision model</strong> with <strong>conditional multimodal reasoning</strong> gives the best of both worlds on mobile: quick answers when the photo is clear and expert-like judgment when species look alike—<strong>without relying on flaky field networks</strong>.</p> <p><strong>What’s next:</strong></p> <ul> <li><strong>Richer cue cards</strong> — concise, visual-first snippets per class (color/venation/antennae/legs).</li> <li><strong>More classes &amp; field data</strong> — expand coverage and robustness with diverse outdoor images.</li> <li><strong>Explainability UI</strong> — “why not <em>this</em> species?” side-by-side comparisons to aid learning and trust.</li> <li><strong>Lightweight field guides</strong> — offline mini-guides for common pests and pollinators.</li> </ul>]]></content><author><name></name></author><category term="computer-vision,"/><category term="deep-learning,"/><category term="insects,"/><category term="education"/><summary type="html"><![CDATA[A walkthrough of how the InsectAgent app blends a classic vision model with a lightweight multimodal model (FastVLM) to identify insects on your iPhone, and only “thinks harder” when it needs to.]]></summary></entry><entry><title type="html">AI for insect detection &amp;amp; classification</title><link href="https://ajaynarayanan.github.io/blog/2025/insect-detection/" rel="alternate" type="text/html" title="AI for insect detection &amp;amp; classification"/><published>2025-04-27T00:00:00+00:00</published><updated>2025-04-27T00:00:00+00:00</updated><id>https://ajaynarayanan.github.io/blog/2025/insect-detection</id><content type="html" xml:base="https://ajaynarayanan.github.io/blog/2025/insect-detection/"><![CDATA[<p>After conducting the <a href="https://insectnet.psu.edu/student-resources/sp25-virtual-workshop-ai-for-insect-detection-classification">AI for Insect Detection &amp; Classification Workshop</a><d-cite key="insectnetworkshop2025"></d-cite> at The Pennsylvania State University, I felt motivated to share some of the key ideas more widely.</p> <p>This blog post is designed for readers with little to no coding experience. The goal is to explain how simple motion detection and modern object detection models like YOLO<d-cite key="redmon2016you"></d-cite> can be used to monitor insects — using visuals and plain language, without diving into heavy math or programming details. Images and videos used in the blog are obtained from the InsectEye<d-cite key="homan2023insecteye"></d-cite> system.</p> <p>I hope you find this exploration both accessible and interesting!</p> <h2 id="introduction">Introduction</h2> <p>Step into a field at dawn, and you’ll hear a quiet symphony of tiny wings.<br/> Some belong to helpful pollinators 🐝, while others are hungry pests 🐛.</p> <p>Catching these insects early can protect crops, reduce chemical sprays, and help us understand how seasons and climate shape insect life. But how do you do that without draining batteries or overwhelming tiny edge devices?</p> <p>We use a <strong>two-step camera system</strong> that’s smart and simple:</p> <ol> <li><strong>Notice insect motion</strong> — “Something moved.”</li> <li><strong>Detect pests with YOLO</strong> — “Which insect is it?”</li> </ol> <p>Let’s dive into how motion spotting and fast object detection help keep a watchful eye on the fields — one tiny wingbeat at a time.</p> <hr/> <h2 id="motion-detection-for-insects">Motion detection for insects</h2> <div class="col-sm mt-3 mt-md-0 text-center"> <img src="/assets/blogs/insect_detection/motion_detection/insect.gif" class="img-fluid rounded z-depth-1" style="transform: rotate(180deg); margin-bottom: 20px;" alt="Insect GIF"/> <br/> <img src="/assets/blogs/insect_detection/motion_detection/motion_masks.gif" class="img-fluid rounded z-depth-1" style="transform: rotate(180deg);" alt="Motion Mask GIF"/> </div> <div class="caption mt-2 text-center"> Fig. 1: Animated GIFs showing the insect video sequence (top) and the corresponding motion masks (bottom). </div> <p>When a camera sits still, most of the scene stays the same. If a handful of pixels suddenly change, that signals <strong>motion</strong>. Here’s a simple background subtraction method to pick out moving insects:</p> <ol> <li> <strong>Build a background model</strong><br/> Watch the scene for a few seconds with no insects, then average those frames to create a ‘clean’ background image. Alternatively, use a single snapshot without any bugs. <div class="text-center my-2" style="line-height: 0;"> <img src="/assets/blogs/insect_detection/motion_detection/background_image.png" class="img-fluid rounded z-depth-1" style="transform: rotate(180deg); display: block; margin: 20 auto;" alt="Background Image"/> <div class="caption text-center" style="margin-top: 20px; font-size: 0.9rem;"> Fig. 2: Background model for our video sequence. </div> </div> </li> <li> <strong>Frame difference</strong><br/> <ul style="margin-top: 0.5rem; margin-bottom: 0;"> <li>Subtract the background model from the current frame to get a difference image.</li> <li>Large pixel differences usually mean motion.</li> <li>Threshold the difference image into a black-and-white motion mask (white = moving pixels; black = background).</li> </ul> </li> </ol> <div class="col-sm mt-3 mt-md-0 text-center"> <div style="background: white; padding: 10px; display: inline-block; border-radius: 8px;"> <img src="/assets/blogs/insect_detection/motion_detection/motion_detection_mask.png" class="img-fluid rounded" style="margin-bottom: 20px;" alt="Motion Detection Mask"/> </div> </div> <div class="caption mt-2 text-center"> Fig. 3: Difference mask obtained by subtracting the current frame from the background and then thresholding. </div> <p>Motion detection is all around us. Here are a few real-world examples:</p> <ul style="margin-top: 0.5rem; margin-bottom: 0;"> <li>Security cameras &amp; intrusion alarms</li> <li>Sports highlights</li> <li>Wildlife monitoring (trail cams)</li> <li>Traffic flow analysis</li> </ul> <hr/> <h2 id="insect-detection-with-yolo">Insect detection with YOLO</h2> <p>Motion detection points out <em>where</em> to look, and <strong>YOLO</strong> tells us <em>what</em> is moving. YOLO<d-cite key="redmon2016you"></d-cite> is an object detection model that draws boxes around objects and labels them. We use YOLO for insects because:</p> <ul> <li><strong>Real-time:</strong> Runs at video speed, even on small devices.</li> <li><strong>All-in-one:</strong> Finds both location and the insect’s identity in a single pass.</li> <li><strong>Open-source:</strong> Free models (YOLOv8) and easy training tools are available.</li> </ul> <p>We now describe the original version of YOLO. Although many improvements have been made across subsequent versions, the core ideas outlined below remain the same.</p> <h3 id="the-quick-yolo-tour">The quick YOLO tour</h3> <ol> <li> <strong>Candidate frames</strong><br/> <em>After motion detection, we extract snapshots that might contain insects.</em> <div class="text-center"> <img src="/assets/blogs/insect_detection/yolo_steps/input_image.jpg" alt="Input Image" class="img-fluid rounded"/> <div class="caption mt-2 text-center"> Fig. 4: Input image for insect detection, obtained from motion detection. </div> </div> </li> <li> <strong>Resize &amp; grid</strong><br/> <em>Resize each snapshot to 448×448 pixels and overlay a 7×7 grid.</em> <div class="text-center"> <img src="/assets/blogs/insect_detection/yolo_steps/cropped_grid_image.png" alt="Cropped and 7x7 grid overlayed image" class="img-fluid rounded"/> <div class="caption mt-2 text-center"> Fig. 5: A 7×7 grid overlaid on the cropped image. </div> </div> </li> <li> <strong>One glance, many predictions</strong><br/> <em>For each grid cell, YOLO’s neural network predicts several bounding boxes, confidence scores, and class probabilities for different insects.</em> <div class="text-center"> <img src="/assets/blogs/insect_detection/yolo_steps/grid_cell_prediction.png" alt="YOLO grid cell predictions" class="img-fluid rounded"/> <div class="caption mt-2 text-center"> Fig. 6: The green box has a high confidence score and class probability; the red boxes are low confidence. </div> </div> </li> <li> <strong>Keep the best</strong><br/> <em>Non-Maximum Suppression (NMS)<d-cite key="hosang2017learning"></d-cite> keeps the highest-scoring box and removes overlapping, lower-scoring ones.</em> <div class="text-center"> <img src="/assets/blogs/insect_detection/yolo_steps/non_maximum_suppression.png" alt="Non Maximum Suppression process" class="img-fluid rounded"/> <div class="caption mt-2 text-center"> Fig. 7: NMS removes duplicate boxes, leaving one clear box per object. </div> </div> <p class="mt-2"> After predicting many boxes, NMS: </p> <ul style="margin-top: 0.5rem; margin-bottom: 0;"> <li>Keeps the box with the highest confidence score.</li> <li>Removes boxes that overlap too much (measured by Intersection over Union, or IoU).</li> </ul> <p> This leaves one clean box per insect. In our diagram, the green box remains and the yellow ones are discarded. </p> </li> <li> <strong>Final output</strong><br/> <em>We get the insect’s class, a confidence score, and the bounding box coordinates.</em> </li> </ol> <p>You might be wondering:</p> <ol> <li> <strong>How are the bounding boxes represented?</strong> <div class="text-center"> <div style="background: white; padding: 10px; display: inline-block; border-radius: 8px;"> <img src="/assets/blogs/insect_detection/yolo_steps/bounding_box_representation.png" alt="Bounding box representation" class="img-fluid rounded"/> </div> <div class="caption mt-2 text-center"> Fig. 8: Bounding box representation. </div> </div> <ul style="margin-top: 0.5rem; margin-bottom: 0;"> <li>Each box is defined by (cx, cy, w, h).</li> <li>(cx, cy) is the box center, normalized within its grid cell.</li> <li>(w, h) is the width and height, relative to the full image.</li> </ul> </li> <li> <strong>What if the insect overlaps two grid cells?</strong> <div class="text-center"> <img src="/assets/blogs/insect_detection/yolo_steps/overlapping_grid_cells.png" alt="Overlapping grid cells" class="img-fluid rounded"/> <div class="caption mt-2 text-center"> Fig. 9: Handling overlapping grid cells. </div> </div> <ul style="margin-top: 10px;"> <li>Only the cell (marked as 1) containing the insect’s center (marked by a star) makes the prediction.</li> <li>Neighboring cells ignore this insect and focus on objects whose centers lie inside them.</li> </ul> </li> </ol> <hr/> <h2 id="conclusion--next-steps">Conclusion &amp; next steps</h2> <p>Combining <strong>motion detection</strong> with <strong>YOLO</strong> creates a nimble tag-team. On a 90,000-frame video, motion detection trimmed the candidates down to just 1,000 frames. This shows why pairing motion and object detection is essential for bio-monitoring systems that run on tight compute and energy budgets.</p> <p>Next on our roadmap:</p> <ul> <li><strong>See even smaller bugs</strong> — add a zoom lens or train YOLO with clearer close-ups.</li> <li><strong>Dashboards for farmers</strong> — stream live counts to a phone app for same-day action.</li> </ul> <p>Have ideas, questions, or cool bug clips? Drop a comment below — we’d love to chat! 🪲👋</p>]]></content><author><name></name></author><category term="computer-vision,"/><category term="deep-learning,"/><category term="insects,"/><category term="education"/><summary type="html"><![CDATA[A simple, visual walkthrough of how AI can spot and identify insects using motion detection and YOLO.]]></summary></entry></feed>